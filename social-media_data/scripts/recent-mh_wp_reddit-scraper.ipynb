{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*'}\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "# ===========================================\n",
    "# TODO (0.1): load subjects into a pandas dataframe\n",
    "# TEST out your api keys(client_id and client_secret):\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"uq9DzHQrG4j0up-QAaS3GA\",\n",
    "    client_secret=\"hkUm1YwxNcU-4rgt_FxXMgEXbrpK0w\",\n",
    "    user_agent=\".\"\n",
    ")\n",
    "\n",
    "print(reddit.auth.scopes())\n",
    "# ===========================================\n",
    "# For read-only authorizations this should return {\"*\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to deaf_mental_health_workplace_recent.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        # Initialize scraper with your Reddit API credentials.\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=\"uq9DzHQrG4j0up-QAaS3GA\",\n",
    "            client_secret=\"hkUm1YwxNcU-4rgt_FxXMgEXbrpK0w\",\n",
    "            user_agent=\".\",\n",
    "            check_for_async=False\n",
    "        )\n",
    "\n",
    "    def search_reddit(self, subreddit, query, limit=100):\n",
    "        \"\"\"\n",
    "        Search the specified subreddit for the most recent posts based on a query.\n",
    "        Args:\n",
    "          subreddit (str): Subreddit to search in (e.g., \"deaf\")\n",
    "          query (str): keyword combination for searching (e.g., \"mental health OR workplace\")\n",
    "          limit (int): Maximum number of posts to fetch (default: 100)\n",
    "        Return:\n",
    "          data: A list of posts with titles, scores, subreddits, and comments\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        # Search in the specified subreddit using the query, sorted by new\n",
    "        fetched_submissions = self.reddit.subreddit(subreddit).search(query, sort='new', limit=limit)\n",
    "        for submission in fetched_submissions:\n",
    "            submission_data = {\n",
    "                'title': submission.title,\n",
    "                'score': submission.score,\n",
    "                'subreddit': str(submission.subreddit),\n",
    "                'comments': []\n",
    "            }\n",
    "\n",
    "            submission.comments.replace_more(limit=None)  # Unpack all comments\n",
    "            for comment in submission.comments.list():\n",
    "                # Only include comments that mention \"mental health\" or \"workplace\" (case insensitive)\n",
    "                if 'mental health' in comment.body.lower() or 'workplace' in comment.body.lower():\n",
    "                    submission_data['comments'].append(comment.body)\n",
    "            \n",
    "            # Only append the submission if there are relevant comments\n",
    "            if submission_data['comments']:\n",
    "                data.append(submission_data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_to_csv(self, data, filename=\"reddit_data.csv\"):\n",
    "        \"\"\"\n",
    "        Save the scraped data to a CSV file.\n",
    "        Args:\n",
    "          data: List of dictionaries containing the data\n",
    "          filename (str): Name of the file to save the data in\n",
    "        \"\"\"\n",
    "        # Define CSV headers\n",
    "        csv_headers = ['title', 'score', 'subreddit', 'comments']\n",
    "\n",
    "        # Open the file in write mode and save the data\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "            writer.writeheader()\n",
    "            for entry in data:\n",
    "                # Convert comments list to a single string\n",
    "                entry['comments'] = \"\\n\".join(entry['comments'])\n",
    "                writer.writerow(entry)\n",
    "\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "# Keywords focusing on mental health or workplace topics\n",
    "query = 'mental health OR workplace'\n",
    "\n",
    "scraper = Scraper()\n",
    "\n",
    "# Search in the specific subreddit \"deaf\" for the most recent posts\n",
    "search_results = scraper.search_reddit('deaf', query, limit=100)\n",
    "\n",
    "# Save the results to CSV\n",
    "scraper.save_to_csv(search_results, 'deaf_mental_health_workplace_recent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to blind_mental_health_workplace_recent.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        # Initialize scraper with your Reddit API credentials.\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=\"uq9DzHQrG4j0up-QAaS3GA\",\n",
    "            client_secret=\"hkUm1YwxNcU-4rgt_FxXMgEXbrpK0w\",\n",
    "            user_agent=\".\",\n",
    "            check_for_async=False\n",
    "        )\n",
    "\n",
    "    def search_reddit(self, subreddit, query, limit=100):\n",
    "        \"\"\"\n",
    "        Search the specified subreddit for the most recent posts based on a query.\n",
    "        Args:\n",
    "          subreddit (str): Subreddit to search in (e.g., \"Blind\")\n",
    "          query (str): keyword combination for searching (e.g., \"mental health OR workplace\")\n",
    "          limit (int): Maximum number of posts to fetch (default: 100)\n",
    "        Return:\n",
    "          data: A list of posts with titles, scores, subreddits, and comments\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        # Search in the specified subreddit using the query, sorted by new\n",
    "        fetched_submissions = self.reddit.subreddit(subreddit).search(query, sort='new', limit=limit)\n",
    "        for submission in fetched_submissions:\n",
    "            submission_data = {\n",
    "                'title': submission.title,\n",
    "                'score': submission.score,\n",
    "                'subreddit': str(submission.subreddit),\n",
    "                'comments': []\n",
    "            }\n",
    "\n",
    "            submission.comments.replace_more(limit=None)  # Unpack all comments\n",
    "            for comment in submission.comments.list():\n",
    "                # Only include comments that mention \"mental health\" or \"workplace\" (case insensitive)\n",
    "                if 'mental health' in comment.body.lower() or 'workplace' in comment.body.lower():\n",
    "                    submission_data['comments'].append(comment.body)\n",
    "            \n",
    "            # Only append the submission if there are relevant comments\n",
    "            if submission_data['comments']:\n",
    "                data.append(submission_data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_to_csv(self, data, filename=\"reddit_data.csv\"):\n",
    "        \"\"\"\n",
    "        Save the scraped data to a CSV file.\n",
    "        Args:\n",
    "          data: List of dictionaries containing the data\n",
    "          filename (str): Name of the file to save the data in\n",
    "        \"\"\"\n",
    "        # Define CSV headers\n",
    "        csv_headers = ['title', 'score', 'subreddit', 'comments']\n",
    "\n",
    "        # Open the file in write mode and save the data\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "            writer.writeheader()\n",
    "            for entry in data:\n",
    "                # Convert comments list to a single string\n",
    "                entry['comments'] = \"\\n\".join(entry['comments'])\n",
    "                writer.writerow(entry)\n",
    "\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "# Keywords focusing on mental health or workplace topics\n",
    "query = 'mental health OR workplace'\n",
    "\n",
    "scraper = Scraper()\n",
    "\n",
    "# Search in the specific subreddit \"Blind\" for the most recent posts\n",
    "search_results = scraper.search_reddit('Blind', query, limit=100)\n",
    "\n",
    "# Save the results to CSV\n",
    "scraper.save_to_csv(search_results, 'blind_mental_health_workplace_recent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to disability_mental_health_workplace_recent.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        # Initialize scraper with your Reddit API credentials.\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=\"uq9DzHQrG4j0up-QAaS3GA\",\n",
    "            client_secret=\"hkUm1YwxNcU-4rgt_FxXMgEXbrpK0w\",\n",
    "            user_agent=\".\",\n",
    "            check_for_async=False\n",
    "        )\n",
    "\n",
    "    def search_reddit(self, subreddit, query, limit=100):\n",
    "        \"\"\"\n",
    "        Search the specified subreddit for the most recent posts based on a query.\n",
    "        Args:\n",
    "          subreddit (str): Subreddit to search in (e.g., \"disability\")\n",
    "          query (str): keyword combination for searching (e.g., \"mental health OR workplace\")\n",
    "          limit (int): Maximum number of posts to fetch (default: 100)\n",
    "        Return:\n",
    "          data: A list of posts with titles, scores, subreddits, and comments\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        # Search in the specified subreddit using the query, sorted by new\n",
    "        fetched_submissions = self.reddit.subreddit(subreddit).search(query, sort='new', limit=limit)\n",
    "        for submission in fetched_submissions:\n",
    "            submission_data = {\n",
    "                'title': submission.title,\n",
    "                'score': submission.score,\n",
    "                'subreddit': str(submission.subreddit),\n",
    "                'comments': []\n",
    "            }\n",
    "\n",
    "            submission.comments.replace_more(limit=None)  # Unpack all comments\n",
    "            for comment in submission.comments.list():\n",
    "                # Only include comments that mention \"mental health\" or \"workplace\" (case insensitive)\n",
    "                if 'mental health' in comment.body.lower() or 'workplace' in comment.body.lower():\n",
    "                    submission_data['comments'].append(comment.body)\n",
    "            \n",
    "            # Only append the submission if there are relevant comments\n",
    "            if submission_data['comments']:\n",
    "                data.append(submission_data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_to_csv(self, data, filename=\"reddit_data.csv\"):\n",
    "        \"\"\"\n",
    "        Save the scraped data to a CSV file.\n",
    "        Args:\n",
    "          data: List of dictionaries containing the data\n",
    "          filename (str): Name of the file to save the data in\n",
    "        \"\"\"\n",
    "        # Define CSV headers\n",
    "        csv_headers = ['title', 'score', 'subreddit', 'comments']\n",
    "\n",
    "        # Open the file in write mode and save the data\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "            writer.writeheader()\n",
    "            for entry in data:\n",
    "                # Convert comments list to a single string\n",
    "                entry['comments'] = \"\\n\".join(entry['comments'])\n",
    "                writer.writerow(entry)\n",
    "\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "# Keywords focusing on mental health or workplace topics\n",
    "query = 'mental health OR workplace'\n",
    "\n",
    "scraper = Scraper()\n",
    "\n",
    "# Search in the specific subreddit \"disability\" for the most recent posts\n",
    "search_results = scraper.search_reddit('disability', query, limit=100)\n",
    "\n",
    "# Save the results to CSV\n",
    "scraper.save_to_csv(search_results, 'disability_mental_health_workplace_recent.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
