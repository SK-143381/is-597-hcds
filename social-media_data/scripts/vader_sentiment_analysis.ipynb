{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>type</th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p0001</td>\n",
       "      <td>popular</td>\n",
       "      <td>mh_wp</td>\n",
       "      <td>Apparently, finding interpreters is impossible.</td>\n",
       "      <td>88</td>\n",
       "      <td>deaf</td>\n",
       "      <td>This is absolutely unacceptable.  I am an inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p0002</td>\n",
       "      <td>popular</td>\n",
       "      <td>mh_wp</td>\n",
       "      <td>I hate being deaf</td>\n",
       "      <td>82</td>\n",
       "      <td>deaf</td>\n",
       "      <td>The DMV? I would not have guessed that would b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p0003</td>\n",
       "      <td>popular</td>\n",
       "      <td>mh_wp</td>\n",
       "      <td>rant about this community</td>\n",
       "      <td>62</td>\n",
       "      <td>deaf</td>\n",
       "      <td>I'm so sorry you received so much negative ene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p0004</td>\n",
       "      <td>popular</td>\n",
       "      <td>mh_wp</td>\n",
       "      <td>Mental Health Issues In Deaf Community</td>\n",
       "      <td>63</td>\n",
       "      <td>deaf</td>\n",
       "      <td>I don't remember where I saw this so I can't c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p0005</td>\n",
       "      <td>popular</td>\n",
       "      <td>mh_wp</td>\n",
       "      <td>Relationship advice: deaf husband/hearing wife...</td>\n",
       "      <td>57</td>\n",
       "      <td>deaf</td>\n",
       "      <td>I’m deaf, my husband is hearing. I’m glad you’...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pid     type   code                                              title  \\\n",
       "0  p0001  popular  mh_wp    Apparently, finding interpreters is impossible.   \n",
       "1  p0002  popular  mh_wp                                  I hate being deaf   \n",
       "2  p0003  popular  mh_wp                          rant about this community   \n",
       "3  p0004  popular  mh_wp             Mental Health Issues In Deaf Community   \n",
       "4  p0005  popular  mh_wp  Relationship advice: deaf husband/hearing wife...   \n",
       "\n",
       "   score subreddit                                           comments  \n",
       "0     88      deaf  This is absolutely unacceptable.  I am an inte...  \n",
       "1     82      deaf  The DMV? I would not have guessed that would b...  \n",
       "2     62      deaf  I'm so sorry you received so much negative ene...  \n",
       "3     63      deaf  I don't remember where I saw this so I can't c...  \n",
       "4     57      deaf  I’m deaf, my husband is hearing. I’m glad you’...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided dataset\n",
    "data  = pd.read_csv('combined_all_data_no_duplicates.csv')\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nitya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nitya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>cleaned_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is absolutely unacceptable.  I am an inte...</td>\n",
       "      <td>absolutely unacceptable interpreter worked men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The DMV? I would not have guessed that would b...</td>\n",
       "      <td>dmv would guessed would deaf friendly workplace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm so sorry you received so much negative ene...</td>\n",
       "      <td>im sorry received much negative energy missed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't remember where I saw this so I can't c...</td>\n",
       "      <td>dont remember saw cant cite researching someth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m deaf, my husband is hearing. I’m glad you’...</td>\n",
       "      <td>im deaf husband hearing im glad youre proactiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  \\\n",
       "0  This is absolutely unacceptable.  I am an inte...   \n",
       "1  The DMV? I would not have guessed that would b...   \n",
       "2  I'm so sorry you received so much negative ene...   \n",
       "3  I don't remember where I saw this so I can't c...   \n",
       "4  I’m deaf, my husband is hearing. I’m glad you’...   \n",
       "\n",
       "                                    cleaned_comments  \n",
       "0  absolutely unacceptable interpreter worked men...  \n",
       "1    dmv would guessed would deaf friendly workplace  \n",
       "2  im sorry received much negative energy missed ...  \n",
       "3  dont remember saw cant cite researching someth...  \n",
       "4  im deaf husband hearing im glad youre proactiv...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetical characters, convert to lowercase, tokenize, remove stopwords, and lemmatize\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "    text = text.lower()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing to 'comments'\n",
    "data['cleaned_comments'] = data['comments'].apply(preprocess_text)\n",
    "data[['comments', 'cleaned_comments']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            comments  \\\n",
      "0  This is absolutely unacceptable.  I am an inte...   \n",
      "1  The DMV? I would not have guessed that would b...   \n",
      "2  I'm so sorry you received so much negative ene...   \n",
      "3  I don't remember where I saw this so I can't c...   \n",
      "4  I’m deaf, my husband is hearing. I’m glad you’...   \n",
      "\n",
      "                                    cleaned_comments vader_sentiment  \\\n",
      "0  absolutely unacceptable interpreter worked men...        negative   \n",
      "1    dmv would guessed would deaf friendly workplace        positive   \n",
      "2  im sorry received much negative energy missed ...        positive   \n",
      "3  dont remember saw cant cite researching someth...        positive   \n",
      "4  im deaf husband hearing im glad youre proactiv...        positive   \n",
      "\n",
      "   vader_pos  vader_neu  vader_neg  vader_compound  \n",
      "0      0.149      0.687      0.165         -0.3989  \n",
      "1      0.348      0.652      0.000          0.4939  \n",
      "2      0.209      0.706      0.085          0.9956  \n",
      "3      0.193      0.641      0.166          0.8622  \n",
      "4      0.254      0.598      0.148          0.9301  \n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment_label = 'neutral'\n",
    "    if scores['compound'] >= 0.05:\n",
    "        sentiment_label = 'positive'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        sentiment_label = 'negative'\n",
    "    return scores, sentiment_label\n",
    "\n",
    "data['vader_scores'], data['vader_sentiment'] = zip(*data['cleaned_comments'].apply(analyze_sentiment_vader))\n",
    "\n",
    "# Split out individual sentiment scores\n",
    "data['vader_pos'] = data['vader_scores'].apply(lambda x: x['pos'])\n",
    "data['vader_neu'] = data['vader_scores'].apply(lambda x: x['neu'])\n",
    "data['vader_neg'] = data['vader_scores'].apply(lambda x: x['neg'])\n",
    "data['vader_compound'] = data['vader_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "data = data.drop(columns=['vader_scores'])\n",
    "print(data[['comments', 'cleaned_comments', 'vader_sentiment', 'vader_pos', 'vader_neu', 'vader_neg', 'vader_compound']].head())\n",
    "\n",
    "# Save the results to a CSV file \n",
    "data.to_csv('social-media_data/output_sentiment_analysis/vader_sentiment_analysis_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647058823529411\n",
      "[[ 2  0  8]\n",
      " [ 0  0  1]\n",
      " [ 3  0 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.20      0.27        10\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.80      0.93      0.86        40\n",
      "\n",
      "    accuracy                           0.76        51\n",
      "   macro avg       0.40      0.38      0.38        51\n",
      "weighted avg       0.71      0.76      0.73        51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitya\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nitya\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nitya\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data['sentiment_label'] = data['vader_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "\n",
    "X = data['cleaned_comments']\n",
    "y = data['sentiment_label']\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier with balanced class weight \n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
